{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83415860",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:16.526921Z",
     "iopub.status.busy": "2023-05-06T08:43:16.526474Z",
     "iopub.status.idle": "2023-05-06T08:43:20.914281Z",
     "shell.execute_reply": "2023-05-06T08:43:20.913379Z"
    },
    "papermill": {
     "duration": 4.398148,
     "end_time": "2023-05-06T08:43:20.916480",
     "exception": false,
     "start_time": "2023-05-06T08:43:16.518332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b55727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:20.928313Z",
     "iopub.status.busy": "2023-05-06T08:43:20.927878Z",
     "iopub.status.idle": "2023-05-06T08:43:21.052784Z",
     "shell.execute_reply": "2023-05-06T08:43:21.051865Z"
    },
    "papermill": {
     "duration": 0.132212,
     "end_time": "2023-05-06T08:43:21.054618",
     "exception": false,
     "start_time": "2023-05-06T08:43:20.922406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4d7d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:21.065062Z",
     "iopub.status.busy": "2023-05-06T08:43:21.064363Z",
     "iopub.status.idle": "2023-05-06T08:43:21.298945Z",
     "shell.execute_reply": "2023-05-06T08:43:21.297866Z"
    },
    "papermill": {
     "duration": 0.241679,
     "end_time": "2023-05-06T08:43:21.300755",
     "exception": false,
     "start_time": "2023-05-06T08:43:21.059076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba5c286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:21.311993Z",
     "iopub.status.busy": "2023-05-06T08:43:21.311137Z",
     "iopub.status.idle": "2023-05-06T08:43:21.320985Z",
     "shell.execute_reply": "2023-05-06T08:43:21.320199Z"
    },
    "papermill": {
     "duration": 0.01726,
     "end_time": "2023-05-06T08:43:21.322858",
     "exception": false,
     "start_time": "2023-05-06T08:43:21.305598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NLTK теги частей речи отличаются от WORDNET тегов\n",
    "def get_pos(word: str) -> str:\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_review(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    # удаляем все символы кроме букв латинского алфавита\n",
    "    text = re.sub(r\"[^a-z]\", repl=\" \", string=text, flags=re.MULTILINE)\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    words = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word not in STOPWORDS:  # удаляем стоп-слова до лемматизации - так можно чуть-чуть сэкономить\n",
    "            lemma = lemmatizer.lemmatize(word, pos=get_pos(word))\n",
    "            # удаляем стоп-слова, наивное предположение - не брать леммы короче 3-х символов дало значительный прирост точности\n",
    "            if lemma not in STOPWORDS and len(lemma) > 2:\n",
    "                words.append(lemma)\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead4eb2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:21.333025Z",
     "iopub.status.busy": "2023-05-06T08:43:21.332758Z",
     "iopub.status.idle": "2023-05-06T08:43:21.394269Z",
     "shell.execute_reply": "2023-05-06T08:43:21.393254Z"
    },
    "papermill": {
     "duration": 0.069333,
     "end_time": "2023-05-06T08:43:21.396739",
     "exception": false,
     "start_time": "2023-05-06T08:43:21.327406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e1b9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:21.407908Z",
     "iopub.status.busy": "2023-05-06T08:43:21.407125Z",
     "iopub.status.idle": "2023-05-06T08:43:21.415720Z",
     "shell.execute_reply": "2023-05-06T08:43:21.414858Z"
    },
    "papermill": {
     "duration": 0.016025,
     "end_time": "2023-05-06T08:43:21.417547",
     "exception": false,
     "start_time": "2023-05-06T08:43:21.401522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, positive_path: Path, negative_path: Path, seed: int = None):\n",
    "        self.positive_path = positive_path\n",
    "        self.negative_path = negative_path\n",
    "        self.positive_reviews = self.read_reviews(positive_path, preprocess_review)\n",
    "        self.negative_reviews = self.read_reviews(negative_path, preprocess_review)\n",
    "\n",
    "        data = self.positive_reviews + self.negative_reviews\n",
    "        targets = torch.cat([torch.ones(len(self.positive_reviews)), torch.zeros(len(self.negative_reviews))])\n",
    "\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        indices = torch.randperm(len(data))\n",
    "\n",
    "        self.data = [data[i] for i in indices]\n",
    "        self.targets = targets[indices].to(torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_reviews(path: Path, process: t.Callable[[str], str]) -> list[str]:\n",
    "        reviews = []\n",
    "        with open(path) as f:\n",
    "            for review in f.readlines():\n",
    "                review = process(review)\n",
    "                if review:\n",
    "                    reviews.append(review)\n",
    "        return reviews\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed582a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:21.428390Z",
     "iopub.status.busy": "2023-05-06T08:43:21.427702Z",
     "iopub.status.idle": "2023-05-06T08:43:22.658906Z",
     "shell.execute_reply": "2023-05-06T08:43:22.657856Z"
    },
    "papermill": {
     "duration": 1.23861,
     "end_time": "2023-05-06T08:43:22.660851",
     "exception": false,
     "start_time": "2023-05-06T08:43:21.422241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f63edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:43:22.672516Z",
     "iopub.status.busy": "2023-05-06T08:43:22.672225Z",
     "iopub.status.idle": "2023-05-06T08:53:40.104676Z",
     "shell.execute_reply": "2023-05-06T08:53:40.103791Z"
    },
    "papermill": {
     "duration": 617.444806,
     "end_time": "2023-05-06T08:53:40.110908",
     "exception": false,
     "start_time": "2023-05-06T08:43:22.666102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151809, ('would wish install bad enemy', tensor(0)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_dataset = ReviewsDataset(\n",
    "     \"/kaggle/input/positive-reviews/positive_reviews.txt\",\n",
    "     \"/kaggle/input/negative-reviews/negative_reviews.txt\",\n",
    "    seed=0,\n",
    ")\n",
    "len(reviews_dataset), reviews_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9302accc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:53:40.123351Z",
     "iopub.status.busy": "2023-05-06T08:53:40.121830Z",
     "iopub.status.idle": "2023-05-06T08:53:40.139784Z",
     "shell.execute_reply": "2023-05-06T08:53:40.138853Z"
    },
    "papermill": {
     "duration": 0.025869,
     "end_time": "2023-05-06T08:53:40.141748",
     "exception": false,
     "start_time": "2023-05-06T08:53:40.115879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121447, 30362)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "train_reviews, test_reviews = train_test_split(reviews_dataset, train_part=0.8)\n",
    "len(train_reviews), len(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c8e1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:53:40.152782Z",
     "iopub.status.busy": "2023-05-06T08:53:40.152531Z",
     "iopub.status.idle": "2023-05-06T08:53:40.166491Z",
     "shell.execute_reply": "2023-05-06T08:53:40.165567Z"
    },
    "papermill": {
     "duration": 0.021603,
     "end_time": "2023-05-06T08:53:40.168347",
     "exception": false,
     "start_time": "2023-05-06T08:53:40.146744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewsVocab:\n",
    "    pad = \"<PAD>\"\n",
    "    unknown = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, reviews: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for review in reviews:\n",
    "            words = nltk.word_tokenize(review)\n",
    "            uniques.update(words)\n",
    "            max_len = max(len(words), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, self.unknown, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        w2i = {w: i for i, w in enumerate(self.alphabet)}\n",
    "        # если ключ отсутствует, будет возвращена 1 - индекс служебного символа\n",
    "        self.w2i = defaultdict(lambda: 1, w2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    @lru_cache(maxsize=8192)  # сомнительная эффективность? Ну да\n",
    "    def encode(self, review: str) -> torch.Tensor:\n",
    "        indices = [self.w2i[w] for w in nltk.word_tokenize(review)]\n",
    "        indices += [self.w2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.w2i[self.pad], as_tuple=True)[0]  # noqa\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \" \".join(self.alphabet[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "748c9a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:53:40.179993Z",
     "iopub.status.busy": "2023-05-06T08:53:40.179726Z",
     "iopub.status.idle": "2023-05-06T08:53:59.201635Z",
     "shell.execute_reply": "2023-05-06T08:53:59.200710Z"
    },
    "papermill": {
     "duration": 19.030117,
     "end_time": "2023-05-06T08:53:59.203470",
     "exception": false,
     "start_time": "2023-05-06T08:53:40.173353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet: 59319 longest: 788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([    1,     1,     1, 22118, 53439,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " '<UNK> <UNK> <UNK> neutral review')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ReviewsVocab([review for review, _ in train_reviews])\n",
    "print(f\"alphabet: {len(vocab)}\", f\"longest: {vocab.max_len}\")\n",
    "encoded = vocab.encode(\"this is a neutral review\")\n",
    "encoded, vocab.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c099a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:53:59.215463Z",
     "iopub.status.busy": "2023-05-06T08:53:59.215191Z",
     "iopub.status.idle": "2023-05-06T08:53:59.225106Z",
     "shell.execute_reply": "2023-05-06T08:53:59.224311Z"
    },
    "papermill": {
     "duration": 0.017986,
     "end_time": "2023-05-06T08:53:59.226902",
     "exception": false,
     "start_time": "2023-05-06T08:53:59.208916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewsClassifier(nn.Module):\n",
    "    LAST_CONV_OUT_CHANNELS = 64\n",
    "    ADAPTIVE_AVG_POOL = 8\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super(ReviewsClassifier, self).__init__()\n",
    "\n",
    "        # Как же этой модели все это... безразлично\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=self.LAST_CONV_OUT_CHANNELS, kernel_size=2),\n",
    "            nn.BatchNorm1d(num_features=self.LAST_CONV_OUT_CHANNELS),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        # Единственный полезный (и понятный зачем) слой. Зачем? - Позволяет не думать о размерностях\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(self.ADAPTIVE_AVG_POOL)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.LAST_CONV_OUT_CHANNELS * self.ADAPTIVE_AVG_POOL, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def collate(batch: t.List[t.Tuple[str, torch.Tensor]]) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for x, y in batch:\n",
    "        xs.append(vocab.encode(x))\n",
    "        ys.append(y)\n",
    "    return torch.vstack(xs), torch.hstack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6caf56b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:53:59.239508Z",
     "iopub.status.busy": "2023-05-06T08:53:59.238158Z",
     "iopub.status.idle": "2023-05-06T08:54:02.211912Z",
     "shell.execute_reply": "2023-05-06T08:54:02.211013Z"
    },
    "papermill": {
     "duration": 2.982087,
     "end_time": "2023-05-06T08:54:02.214144",
     "exception": false,
     "start_time": "2023-05-06T08:53:59.232057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "net = ReviewsClassifier(num_embeddings=len(vocab), embedding_dim=128).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.000914092001)  # а почему нет?\n",
    "# будет изменять lr = lr * factor, если на протяжении patience эпох ошибка не менялась более чем на threshold\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode=\"min\",\n",
    "    patience=5,\n",
    "    factor=0.333333,\n",
    "    min_lr=0.000001,\n",
    "    threshold=0.001,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Однако batch_size - очень важно. При 8 и 64 застреваем в плохом оптимуме, 32 тоже не очень...\n",
    "# 22 > Embedding, Conv1d, BatchNorm1d, ReLU, MaxPool1d, AdaptiveAvgPool1d, Dropout, lr_scheduler, optimizer\n",
    "train_dataloader = DataLoader(train_reviews, batch_size=22, collate_fn=collate, shuffle=True)\n",
    "test_dataloader = DataLoader(test_reviews, batch_size=512, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c929f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:54:02.226503Z",
     "iopub.status.busy": "2023-05-06T08:54:02.226226Z",
     "iopub.status.idle": "2023-05-06T08:54:02.247242Z",
     "shell.execute_reply": "2023-05-06T08:54:02.246355Z"
    },
    "papermill": {
     "duration": 0.029449,
     "end_time": "2023-05-06T08:54:02.249157",
     "exception": false,
     "start_time": "2023-05-06T08:54:02.219708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        lr_scheduler=None,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "        if test_dataloader:\n",
    "            loss, acc = test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce515bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T08:54:02.260783Z",
     "iopub.status.busy": "2023-05-06T08:54:02.260519Z",
     "iopub.status.idle": "2023-05-06T09:19:30.335824Z",
     "shell.execute_reply": "2023-05-06T09:19:30.334862Z"
    },
    "papermill": {
     "duration": 1528.084439,
     "end_time": "2023-05-06T09:19:30.338857",
     "exception": false,
     "start_time": "2023-05-06T08:54:02.254418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 0.688263  [    0/121447]\n",
      "loss: 0.669989  [ 3300/121447]\n",
      "loss: 0.602634  [ 6600/121447]\n",
      "loss: 0.726695  [ 9900/121447]\n",
      "loss: 0.707313  [13200/121447]\n",
      "loss: 0.672009  [16500/121447]\n",
      "loss: 0.665081  [19800/121447]\n",
      "loss: 0.678214  [23100/121447]\n",
      "loss: 0.636483  [26400/121447]\n",
      "loss: 0.685687  [29700/121447]\n",
      "loss: 0.671561  [33000/121447]\n",
      "loss: 0.632208  [36300/121447]\n",
      "loss: 0.532886  [39600/121447]\n",
      "loss: 0.513744  [42900/121447]\n",
      "loss: 0.456883  [46200/121447]\n",
      "loss: 0.459239  [49500/121447]\n",
      "loss: 0.326138  [52800/121447]\n",
      "loss: 0.545915  [56100/121447]\n",
      "loss: 0.301607  [59400/121447]\n",
      "loss: 0.475084  [62700/121447]\n",
      "loss: 0.411620  [66000/121447]\n",
      "loss: 0.483227  [69300/121447]\n",
      "loss: 0.479000  [72600/121447]\n",
      "loss: 0.429365  [75900/121447]\n",
      "loss: 0.537460  [79200/121447]\n",
      "loss: 0.423303  [82500/121447]\n",
      "loss: 0.474779  [85800/121447]\n",
      "loss: 0.736515  [89100/121447]\n",
      "loss: 0.439815  [92400/121447]\n",
      "loss: 0.422479  [95700/121447]\n",
      "loss: 0.326826  [99000/121447]\n",
      "loss: 0.580811  [102300/121447]\n",
      "loss: 0.473252  [105600/121447]\n",
      "loss: 0.625549  [108900/121447]\n",
      "loss: 0.381521  [112200/121447]\n",
      "loss: 0.366538  [115500/121447]\n",
      "loss: 0.477179  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.796324, Avg loss: 0.421946 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 0.461629  [    0/121447]\n",
      "loss: 0.249132  [ 3300/121447]\n",
      "loss: 0.424055  [ 6600/121447]\n",
      "loss: 0.272980  [ 9900/121447]\n",
      "loss: 0.514246  [13200/121447]\n",
      "loss: 0.414600  [16500/121447]\n",
      "loss: 0.446218  [19800/121447]\n",
      "loss: 0.307418  [23100/121447]\n",
      "loss: 0.608187  [26400/121447]\n",
      "loss: 0.238530  [29700/121447]\n",
      "loss: 0.567773  [33000/121447]\n",
      "loss: 0.359280  [36300/121447]\n",
      "loss: 0.340692  [39600/121447]\n",
      "loss: 0.482266  [42900/121447]\n",
      "loss: 0.298822  [46200/121447]\n",
      "loss: 0.324249  [49500/121447]\n",
      "loss: 0.288904  [52800/121447]\n",
      "loss: 0.380425  [56100/121447]\n",
      "loss: 0.459236  [59400/121447]\n",
      "loss: 0.490443  [62700/121447]\n",
      "loss: 0.313327  [66000/121447]\n",
      "loss: 0.266777  [69300/121447]\n",
      "loss: 0.328788  [72600/121447]\n",
      "loss: 0.364265  [75900/121447]\n",
      "loss: 0.267413  [79200/121447]\n",
      "loss: 0.384055  [82500/121447]\n",
      "loss: 0.425597  [85800/121447]\n",
      "loss: 0.701119  [89100/121447]\n",
      "loss: 0.370167  [92400/121447]\n",
      "loss: 0.459936  [95700/121447]\n",
      "loss: 0.487527  [99000/121447]\n",
      "loss: 0.436364  [102300/121447]\n",
      "loss: 0.324147  [105600/121447]\n",
      "loss: 0.496472  [108900/121447]\n",
      "loss: 0.752818  [112200/121447]\n",
      "loss: 0.351817  [115500/121447]\n",
      "loss: 0.291419  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.814867, Avg loss: 0.392747 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.329035  [    0/121447]\n",
      "loss: 0.571518  [ 3300/121447]\n",
      "loss: 0.361086  [ 6600/121447]\n",
      "loss: 0.648349  [ 9900/121447]\n",
      "loss: 0.395681  [13200/121447]\n",
      "loss: 0.397631  [16500/121447]\n",
      "loss: 0.370100  [19800/121447]\n",
      "loss: 0.617044  [23100/121447]\n",
      "loss: 0.345654  [26400/121447]\n",
      "loss: 0.340220  [29700/121447]\n",
      "loss: 0.261043  [33000/121447]\n",
      "loss: 0.682112  [36300/121447]\n",
      "loss: 0.299725  [39600/121447]\n",
      "loss: 0.450293  [42900/121447]\n",
      "loss: 0.519214  [46200/121447]\n",
      "loss: 0.362575  [49500/121447]\n",
      "loss: 0.385339  [52800/121447]\n",
      "loss: 0.475877  [56100/121447]\n",
      "loss: 0.601462  [59400/121447]\n",
      "loss: 0.443581  [62700/121447]\n",
      "loss: 0.182124  [66000/121447]\n",
      "loss: 0.499404  [69300/121447]\n",
      "loss: 0.322164  [72600/121447]\n",
      "loss: 0.299838  [75900/121447]\n",
      "loss: 0.353797  [79200/121447]\n",
      "loss: 0.335120  [82500/121447]\n",
      "loss: 1.059819  [85800/121447]\n",
      "loss: 0.311332  [89100/121447]\n",
      "loss: 0.352459  [92400/121447]\n",
      "loss: 0.432830  [95700/121447]\n",
      "loss: 0.683949  [99000/121447]\n",
      "loss: 0.368035  [102300/121447]\n",
      "loss: 0.404163  [105600/121447]\n",
      "loss: 0.354075  [108900/121447]\n",
      "loss: 0.429632  [112200/121447]\n",
      "loss: 0.497883  [115500/121447]\n",
      "loss: 0.377564  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.824682, Avg loss: 0.379601 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.442323  [    0/121447]\n",
      "loss: 0.276229  [ 3300/121447]\n",
      "loss: 0.341940  [ 6600/121447]\n",
      "loss: 0.582192  [ 9900/121447]\n",
      "loss: 0.366235  [13200/121447]\n",
      "loss: 0.370205  [16500/121447]\n",
      "loss: 0.418317  [19800/121447]\n",
      "loss: 0.241708  [23100/121447]\n",
      "loss: 0.403595  [26400/121447]\n",
      "loss: 0.245969  [29700/121447]\n",
      "loss: 0.385321  [33000/121447]\n",
      "loss: 0.557267  [36300/121447]\n",
      "loss: 0.429718  [39600/121447]\n",
      "loss: 0.312290  [42900/121447]\n",
      "loss: 0.598497  [46200/121447]\n",
      "loss: 0.562921  [49500/121447]\n",
      "loss: 0.243869  [52800/121447]\n",
      "loss: 0.411452  [56100/121447]\n",
      "loss: 0.987552  [59400/121447]\n",
      "loss: 0.295374  [62700/121447]\n",
      "loss: 0.507715  [66000/121447]\n",
      "loss: 0.396937  [69300/121447]\n",
      "loss: 0.417203  [72600/121447]\n",
      "loss: 0.448335  [75900/121447]\n",
      "loss: 0.424588  [79200/121447]\n",
      "loss: 0.406988  [82500/121447]\n",
      "loss: 0.492235  [85800/121447]\n",
      "loss: 0.381746  [89100/121447]\n",
      "loss: 0.309729  [92400/121447]\n",
      "loss: 0.194528  [95700/121447]\n",
      "loss: 0.358794  [99000/121447]\n",
      "loss: 0.696495  [102300/121447]\n",
      "loss: 0.312320  [105600/121447]\n",
      "loss: 0.388680  [108900/121447]\n",
      "loss: 0.366195  [112200/121447]\n",
      "loss: 0.380699  [115500/121447]\n",
      "loss: 0.275611  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.839536, Avg loss: 0.363856 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.458222  [    0/121447]\n",
      "loss: 0.364571  [ 3300/121447]\n",
      "loss: 0.281198  [ 6600/121447]\n",
      "loss: 0.303963  [ 9900/121447]\n",
      "loss: 0.296539  [13200/121447]\n",
      "loss: 0.317504  [16500/121447]\n",
      "loss: 0.280703  [19800/121447]\n",
      "loss: 0.333411  [23100/121447]\n",
      "loss: 0.385937  [26400/121447]\n",
      "loss: 0.663441  [29700/121447]\n",
      "loss: 0.433260  [33000/121447]\n",
      "loss: 0.743737  [36300/121447]\n",
      "loss: 0.586824  [39600/121447]\n",
      "loss: 0.324463  [42900/121447]\n",
      "loss: 0.220167  [46200/121447]\n",
      "loss: 0.200883  [49500/121447]\n",
      "loss: 0.618758  [52800/121447]\n",
      "loss: 0.476568  [56100/121447]\n",
      "loss: 0.287887  [59400/121447]\n",
      "loss: 0.454940  [62700/121447]\n",
      "loss: 0.277752  [66000/121447]\n",
      "loss: 0.404784  [69300/121447]\n",
      "loss: 0.221534  [72600/121447]\n",
      "loss: 0.694857  [75900/121447]\n",
      "loss: 0.345261  [79200/121447]\n",
      "loss: 0.394369  [82500/121447]\n",
      "loss: 0.307632  [85800/121447]\n",
      "loss: 0.420190  [89100/121447]\n",
      "loss: 0.583911  [92400/121447]\n",
      "loss: 0.226948  [95700/121447]\n",
      "loss: 0.374581  [99000/121447]\n",
      "loss: 0.664641  [102300/121447]\n",
      "loss: 0.395021  [105600/121447]\n",
      "loss: 0.194939  [108900/121447]\n",
      "loss: 0.236601  [112200/121447]\n",
      "loss: 0.419730  [115500/121447]\n",
      "loss: 0.241071  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.838153, Avg loss: 0.370333 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.329611  [    0/121447]\n",
      "loss: 0.242720  [ 3300/121447]\n",
      "loss: 0.208555  [ 6600/121447]\n",
      "loss: 0.305388  [ 9900/121447]\n",
      "loss: 0.850540  [13200/121447]\n",
      "loss: 0.323091  [16500/121447]\n",
      "loss: 0.801352  [19800/121447]\n",
      "loss: 0.619505  [23100/121447]\n",
      "loss: 0.223699  [26400/121447]\n",
      "loss: 0.307379  [29700/121447]\n",
      "loss: 0.172502  [33000/121447]\n",
      "loss: 0.220774  [36300/121447]\n",
      "loss: 0.785084  [39600/121447]\n",
      "loss: 0.473860  [42900/121447]\n",
      "loss: 0.306954  [46200/121447]\n",
      "loss: 0.283325  [49500/121447]\n",
      "loss: 0.316521  [52800/121447]\n",
      "loss: 0.311674  [56100/121447]\n",
      "loss: 0.304876  [59400/121447]\n",
      "loss: 0.221408  [62700/121447]\n",
      "loss: 0.300869  [66000/121447]\n",
      "loss: 0.456582  [69300/121447]\n",
      "loss: 0.367984  [72600/121447]\n",
      "loss: 0.319470  [75900/121447]\n",
      "loss: 0.262829  [79200/121447]\n",
      "loss: 0.322155  [82500/121447]\n",
      "loss: 0.375848  [85800/121447]\n",
      "loss: 0.493787  [89100/121447]\n",
      "loss: 0.252564  [92400/121447]\n",
      "loss: 0.221457  [95700/121447]\n",
      "loss: 0.147468  [99000/121447]\n",
      "loss: 0.267266  [102300/121447]\n",
      "loss: 0.425346  [105600/121447]\n",
      "loss: 0.208815  [108900/121447]\n",
      "loss: 0.329163  [112200/121447]\n",
      "loss: 0.375049  [115500/121447]\n",
      "loss: 0.482424  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.839701, Avg loss: 0.367195 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.416573  [    0/121447]\n",
      "loss: 0.198450  [ 3300/121447]\n",
      "loss: 0.321194  [ 6600/121447]\n",
      "loss: 0.519835  [ 9900/121447]\n",
      "loss: 0.651163  [13200/121447]\n",
      "loss: 0.383660  [16500/121447]\n",
      "loss: 0.198685  [19800/121447]\n",
      "loss: 0.661568  [23100/121447]\n",
      "loss: 0.171408  [26400/121447]\n",
      "loss: 0.401910  [29700/121447]\n",
      "loss: 0.221021  [33000/121447]\n",
      "loss: 0.545583  [36300/121447]\n",
      "loss: 0.177739  [39600/121447]\n",
      "loss: 0.189308  [42900/121447]\n",
      "loss: 0.185276  [46200/121447]\n",
      "loss: 0.283920  [49500/121447]\n",
      "loss: 0.426906  [52800/121447]\n",
      "loss: 0.312400  [56100/121447]\n",
      "loss: 0.310212  [59400/121447]\n",
      "loss: 0.248389  [62700/121447]\n",
      "loss: 0.272815  [66000/121447]\n",
      "loss: 0.503871  [69300/121447]\n",
      "loss: 0.637934  [72600/121447]\n",
      "loss: 0.236579  [75900/121447]\n",
      "loss: 0.316777  [79200/121447]\n",
      "loss: 0.334351  [82500/121447]\n",
      "loss: 0.560138  [85800/121447]\n",
      "loss: 0.468235  [89100/121447]\n",
      "loss: 0.318396  [92400/121447]\n",
      "loss: 0.177699  [95700/121447]\n",
      "loss: 0.384985  [99000/121447]\n",
      "loss: 0.228663  [102300/121447]\n",
      "loss: 0.292436  [105600/121447]\n",
      "loss: 0.206537  [108900/121447]\n",
      "loss: 0.407980  [112200/121447]\n",
      "loss: 0.547719  [115500/121447]\n",
      "loss: 0.154807  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.837791, Avg loss: 0.369221 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.255236  [    0/121447]\n",
      "loss: 0.240705  [ 3300/121447]\n",
      "loss: 0.164539  [ 6600/121447]\n",
      "loss: 0.222249  [ 9900/121447]\n",
      "loss: 0.109831  [13200/121447]\n",
      "loss: 0.283048  [16500/121447]\n",
      "loss: 0.568183  [19800/121447]\n",
      "loss: 0.243743  [23100/121447]\n",
      "loss: 0.180322  [26400/121447]\n",
      "loss: 0.565041  [29700/121447]\n",
      "loss: 0.310075  [33000/121447]\n",
      "loss: 0.307261  [36300/121447]\n",
      "loss: 0.253033  [39600/121447]\n",
      "loss: 0.287069  [42900/121447]\n",
      "loss: 0.232184  [46200/121447]\n",
      "loss: 0.316063  [49500/121447]\n",
      "loss: 0.616046  [52800/121447]\n",
      "loss: 0.321977  [56100/121447]\n",
      "loss: 0.282315  [59400/121447]\n",
      "loss: 0.315308  [62700/121447]\n",
      "loss: 0.472095  [66000/121447]\n",
      "loss: 0.304965  [69300/121447]\n",
      "loss: 0.119384  [72600/121447]\n",
      "loss: 0.266645  [75900/121447]\n",
      "loss: 0.200428  [79200/121447]\n",
      "loss: 0.289634  [82500/121447]\n",
      "loss: 0.197074  [85800/121447]\n",
      "loss: 0.342025  [89100/121447]\n",
      "loss: 0.584470  [92400/121447]\n",
      "loss: 0.525936  [95700/121447]\n",
      "loss: 0.437655  [99000/121447]\n",
      "loss: 0.242478  [102300/121447]\n",
      "loss: 0.328832  [105600/121447]\n",
      "loss: 0.194539  [108900/121447]\n",
      "loss: 0.450235  [112200/121447]\n",
      "loss: 0.230038  [115500/121447]\n",
      "loss: 0.270943  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.828437, Avg loss: 0.418619 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.150085  [    0/121447]\n",
      "loss: 0.196340  [ 3300/121447]\n",
      "loss: 0.376876  [ 6600/121447]\n",
      "loss: 0.293662  [ 9900/121447]\n",
      "loss: 0.183238  [13200/121447]\n",
      "loss: 0.291025  [16500/121447]\n",
      "loss: 0.280699  [19800/121447]\n",
      "loss: 0.482366  [23100/121447]\n",
      "loss: 0.420245  [26400/121447]\n",
      "loss: 0.375293  [29700/121447]\n",
      "loss: 0.151592  [33000/121447]\n",
      "loss: 0.335453  [36300/121447]\n",
      "loss: 0.298560  [39600/121447]\n",
      "loss: 0.208182  [42900/121447]\n",
      "loss: 0.305367  [46200/121447]\n",
      "loss: 0.541389  [49500/121447]\n",
      "loss: 0.629041  [52800/121447]\n",
      "loss: 0.198538  [56100/121447]\n",
      "loss: 0.437132  [59400/121447]\n",
      "loss: 0.227277  [62700/121447]\n",
      "loss: 0.401287  [66000/121447]\n",
      "loss: 0.291393  [69300/121447]\n",
      "loss: 0.272105  [72600/121447]\n",
      "loss: 0.388210  [75900/121447]\n",
      "loss: 0.907634  [79200/121447]\n",
      "loss: 0.286976  [82500/121447]\n",
      "loss: 0.162701  [85800/121447]\n",
      "loss: 0.433384  [89100/121447]\n",
      "loss: 0.128706  [92400/121447]\n",
      "loss: 0.385605  [95700/121447]\n",
      "loss: 0.246036  [99000/121447]\n",
      "loss: 0.207908  [102300/121447]\n",
      "loss: 0.302159  [105600/121447]\n",
      "loss: 0.139861  [108900/121447]\n",
      "loss: 0.291559  [112200/121447]\n",
      "loss: 0.348162  [115500/121447]\n",
      "loss: 0.445241  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.833740, Avg loss: 0.384693 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.189703  [    0/121447]\n",
      "loss: 0.197736  [ 3300/121447]\n",
      "loss: 0.420049  [ 6600/121447]\n",
      "loss: 0.230019  [ 9900/121447]\n",
      "loss: 0.153021  [13200/121447]\n",
      "loss: 0.210442  [16500/121447]\n",
      "loss: 0.339846  [19800/121447]\n",
      "loss: 0.204273  [23100/121447]\n",
      "loss: 0.411789  [26400/121447]\n",
      "loss: 0.256882  [29700/121447]\n",
      "loss: 0.132261  [33000/121447]\n",
      "loss: 0.091657  [36300/121447]\n",
      "loss: 0.449575  [39600/121447]\n",
      "loss: 0.392461  [42900/121447]\n",
      "loss: 0.307960  [46200/121447]\n",
      "loss: 0.185927  [49500/121447]\n",
      "loss: 0.518014  [52800/121447]\n",
      "loss: 0.270815  [56100/121447]\n",
      "loss: 0.059511  [59400/121447]\n",
      "loss: 0.231896  [62700/121447]\n",
      "loss: 0.500478  [66000/121447]\n",
      "loss: 0.194284  [69300/121447]\n",
      "loss: 0.236385  [72600/121447]\n",
      "loss: 0.360645  [75900/121447]\n",
      "loss: 0.232248  [79200/121447]\n",
      "loss: 0.214221  [82500/121447]\n",
      "loss: 0.429220  [85800/121447]\n",
      "loss: 0.282020  [89100/121447]\n",
      "loss: 0.224221  [92400/121447]\n",
      "loss: 0.231928  [95700/121447]\n",
      "loss: 0.249130  [99000/121447]\n",
      "loss: 0.230986  [102300/121447]\n",
      "loss: 0.333923  [105600/121447]\n",
      "loss: 0.269303  [108900/121447]\n",
      "loss: 0.619586  [112200/121447]\n",
      "loss: 0.350758  [115500/121447]\n",
      "loss: 0.627546  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.836572, Avg loss: 0.432925 \n",
      "\n",
      "Epoch 00010: reducing learning rate of group 0 to 3.0470e-04.\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.174534  [    0/121447]\n",
      "loss: 0.241285  [ 3300/121447]\n",
      "loss: 0.171100  [ 6600/121447]\n",
      "loss: 0.226531  [ 9900/121447]\n",
      "loss: 0.558160  [13200/121447]\n",
      "loss: 0.354310  [16500/121447]\n",
      "loss: 0.126625  [19800/121447]\n",
      "loss: 0.367153  [23100/121447]\n",
      "loss: 0.399026  [26400/121447]\n",
      "loss: 0.182917  [29700/121447]\n",
      "loss: 0.247656  [33000/121447]\n",
      "loss: 0.143432  [36300/121447]\n",
      "loss: 0.124782  [39600/121447]\n",
      "loss: 0.207468  [42900/121447]\n",
      "loss: 0.107095  [46200/121447]\n",
      "loss: 0.206714  [49500/121447]\n",
      "loss: 0.331834  [52800/121447]\n",
      "loss: 0.446831  [56100/121447]\n",
      "loss: 0.251606  [59400/121447]\n",
      "loss: 0.211959  [62700/121447]\n",
      "loss: 0.364583  [66000/121447]\n",
      "loss: 0.203591  [69300/121447]\n",
      "loss: 0.205680  [72600/121447]\n",
      "loss: 0.484300  [75900/121447]\n",
      "loss: 0.459596  [79200/121447]\n",
      "loss: 0.137772  [82500/121447]\n",
      "loss: 0.247719  [85800/121447]\n",
      "loss: 0.073291  [89100/121447]\n",
      "loss: 0.513407  [92400/121447]\n",
      "loss: 0.306420  [95700/121447]\n",
      "loss: 0.261186  [99000/121447]\n",
      "loss: 0.263844  [102300/121447]\n",
      "loss: 0.358837  [105600/121447]\n",
      "loss: 0.158772  [108900/121447]\n",
      "loss: 0.709934  [112200/121447]\n",
      "loss: 0.494263  [115500/121447]\n",
      "loss: 0.146837  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.820236, Avg loss: 0.439013 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.266878  [    0/121447]\n",
      "loss: 0.231740  [ 3300/121447]\n",
      "loss: 0.299995  [ 6600/121447]\n",
      "loss: 0.184411  [ 9900/121447]\n",
      "loss: 0.431566  [13200/121447]\n",
      "loss: 0.120968  [16500/121447]\n",
      "loss: 0.427201  [19800/121447]\n",
      "loss: 0.369823  [23100/121447]\n",
      "loss: 0.354137  [26400/121447]\n",
      "loss: 0.156653  [29700/121447]\n",
      "loss: 0.170657  [33000/121447]\n",
      "loss: 0.253657  [36300/121447]\n",
      "loss: 0.254811  [39600/121447]\n",
      "loss: 0.313834  [42900/121447]\n",
      "loss: 0.270488  [46200/121447]\n",
      "loss: 0.226709  [49500/121447]\n",
      "loss: 0.218909  [52800/121447]\n",
      "loss: 0.207096  [56100/121447]\n",
      "loss: 0.090843  [59400/121447]\n",
      "loss: 0.236276  [62700/121447]\n",
      "loss: 0.179442  [66000/121447]\n",
      "loss: 0.195976  [69300/121447]\n",
      "loss: 0.246613  [72600/121447]\n",
      "loss: 0.291760  [75900/121447]\n",
      "loss: 0.267791  [79200/121447]\n",
      "loss: 0.453309  [82500/121447]\n",
      "loss: 0.301313  [85800/121447]\n",
      "loss: 0.461091  [89100/121447]\n",
      "loss: 0.289030  [92400/121447]\n",
      "loss: 0.084042  [95700/121447]\n",
      "loss: 0.267077  [99000/121447]\n",
      "loss: 0.171851  [102300/121447]\n",
      "loss: 0.191177  [105600/121447]\n",
      "loss: 0.361812  [108900/121447]\n",
      "loss: 0.265022  [112200/121447]\n",
      "loss: 0.227697  [115500/121447]\n",
      "loss: 0.209717  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.820565, Avg loss: 0.430993 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.161550  [    0/121447]\n",
      "loss: 0.238779  [ 3300/121447]\n",
      "loss: 0.246874  [ 6600/121447]\n",
      "loss: 0.405425  [ 9900/121447]\n",
      "loss: 0.208637  [13200/121447]\n",
      "loss: 0.101676  [16500/121447]\n",
      "loss: 0.304974  [19800/121447]\n",
      "loss: 0.244002  [23100/121447]\n",
      "loss: 0.150018  [26400/121447]\n",
      "loss: 0.292584  [29700/121447]\n",
      "loss: 0.289841  [33000/121447]\n",
      "loss: 0.100724  [36300/121447]\n",
      "loss: 0.267390  [39600/121447]\n",
      "loss: 0.172739  [42900/121447]\n",
      "loss: 0.835554  [46200/121447]\n",
      "loss: 0.123363  [49500/121447]\n",
      "loss: 0.242977  [52800/121447]\n",
      "loss: 0.694696  [56100/121447]\n",
      "loss: 0.412749  [59400/121447]\n",
      "loss: 0.248142  [62700/121447]\n",
      "loss: 0.464505  [66000/121447]\n",
      "loss: 0.375948  [69300/121447]\n",
      "loss: 0.637330  [72600/121447]\n",
      "loss: 0.293066  [75900/121447]\n",
      "loss: 0.266905  [79200/121447]\n",
      "loss: 0.362858  [82500/121447]\n",
      "loss: 0.246169  [85800/121447]\n",
      "loss: 0.331676  [89100/121447]\n",
      "loss: 0.107263  [92400/121447]\n",
      "loss: 0.225798  [95700/121447]\n",
      "loss: 0.282483  [99000/121447]\n",
      "loss: 0.209885  [102300/121447]\n",
      "loss: 0.103331  [105600/121447]\n",
      "loss: 0.191426  [108900/121447]\n",
      "loss: 0.244376  [112200/121447]\n",
      "loss: 0.114473  [115500/121447]\n",
      "loss: 0.383337  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.833772, Avg loss: 0.468258 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.113166  [    0/121447]\n",
      "loss: 0.230673  [ 3300/121447]\n",
      "loss: 0.287368  [ 6600/121447]\n",
      "loss: 0.087899  [ 9900/121447]\n",
      "loss: 0.298259  [13200/121447]\n",
      "loss: 0.326617  [16500/121447]\n",
      "loss: 0.264742  [19800/121447]\n",
      "loss: 0.206960  [23100/121447]\n",
      "loss: 0.201634  [26400/121447]\n",
      "loss: 0.159322  [29700/121447]\n",
      "loss: 0.100631  [33000/121447]\n",
      "loss: 0.156878  [36300/121447]\n",
      "loss: 0.126284  [39600/121447]\n",
      "loss: 0.264863  [42900/121447]\n",
      "loss: 0.239981  [46200/121447]\n",
      "loss: 0.260801  [49500/121447]\n",
      "loss: 0.221436  [52800/121447]\n",
      "loss: 0.148748  [56100/121447]\n",
      "loss: 0.131103  [59400/121447]\n",
      "loss: 0.306771  [62700/121447]\n",
      "loss: 0.115559  [66000/121447]\n",
      "loss: 0.122213  [69300/121447]\n",
      "loss: 0.294287  [72600/121447]\n",
      "loss: 0.234552  [75900/121447]\n",
      "loss: 0.302670  [79200/121447]\n",
      "loss: 0.182595  [82500/121447]\n",
      "loss: 0.303180  [85800/121447]\n",
      "loss: 0.032887  [89100/121447]\n",
      "loss: 0.115608  [92400/121447]\n",
      "loss: 0.271824  [95700/121447]\n",
      "loss: 0.199805  [99000/121447]\n",
      "loss: 0.087198  [102300/121447]\n",
      "loss: 0.425711  [105600/121447]\n",
      "loss: 0.210049  [108900/121447]\n",
      "loss: 0.200979  [112200/121447]\n",
      "loss: 0.359408  [115500/121447]\n",
      "loss: 0.284351  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.825012, Avg loss: 0.467512 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.115620  [    0/121447]\n",
      "loss: 0.476287  [ 3300/121447]\n",
      "loss: 0.235285  [ 6600/121447]\n",
      "loss: 0.188430  [ 9900/121447]\n",
      "loss: 0.118596  [13200/121447]\n",
      "loss: 0.202860  [16500/121447]\n",
      "loss: 0.242861  [19800/121447]\n",
      "loss: 0.695904  [23100/121447]\n",
      "loss: 0.178375  [26400/121447]\n",
      "loss: 0.169660  [29700/121447]\n",
      "loss: 0.273795  [33000/121447]\n",
      "loss: 0.146609  [36300/121447]\n",
      "loss: 0.249681  [39600/121447]\n",
      "loss: 0.097899  [42900/121447]\n",
      "loss: 0.274764  [46200/121447]\n",
      "loss: 0.174261  [49500/121447]\n",
      "loss: 0.352361  [52800/121447]\n",
      "loss: 0.158306  [56100/121447]\n",
      "loss: 0.168054  [59400/121447]\n",
      "loss: 0.167300  [62700/121447]\n",
      "loss: 0.239529  [66000/121447]\n",
      "loss: 0.222609  [69300/121447]\n",
      "loss: 0.148498  [72600/121447]\n",
      "loss: 0.456214  [75900/121447]\n",
      "loss: 0.117668  [79200/121447]\n",
      "loss: 0.179115  [82500/121447]\n",
      "loss: 0.166629  [85800/121447]\n",
      "loss: 0.087866  [89100/121447]\n",
      "loss: 0.288126  [92400/121447]\n",
      "loss: 0.239435  [95700/121447]\n",
      "loss: 0.192273  [99000/121447]\n",
      "loss: 0.353841  [102300/121447]\n",
      "loss: 0.217631  [105600/121447]\n",
      "loss: 0.175424  [108900/121447]\n",
      "loss: 0.232625  [112200/121447]\n",
      "loss: 0.113665  [115500/121447]\n",
      "loss: 0.159998  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.823826, Avg loss: 0.485164 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.311814  [    0/121447]\n",
      "loss: 0.152309  [ 3300/121447]\n",
      "loss: 0.086517  [ 6600/121447]\n",
      "loss: 0.137208  [ 9900/121447]\n",
      "loss: 0.270863  [13200/121447]\n",
      "loss: 0.115199  [16500/121447]\n",
      "loss: 0.121047  [19800/121447]\n",
      "loss: 0.162729  [23100/121447]\n",
      "loss: 0.162333  [26400/121447]\n",
      "loss: 0.165006  [29700/121447]\n",
      "loss: 0.166925  [33000/121447]\n",
      "loss: 0.119602  [36300/121447]\n",
      "loss: 0.140900  [39600/121447]\n",
      "loss: 0.371692  [42900/121447]\n",
      "loss: 0.619404  [46200/121447]\n",
      "loss: 0.384231  [49500/121447]\n",
      "loss: 0.195875  [52800/121447]\n",
      "loss: 0.125265  [56100/121447]\n",
      "loss: 0.134813  [59400/121447]\n",
      "loss: 0.133217  [62700/121447]\n",
      "loss: 0.173101  [66000/121447]\n",
      "loss: 0.225725  [69300/121447]\n",
      "loss: 0.252889  [72600/121447]\n",
      "loss: 0.121270  [75900/121447]\n",
      "loss: 0.167864  [79200/121447]\n",
      "loss: 0.205225  [82500/121447]\n",
      "loss: 0.165781  [85800/121447]\n",
      "loss: 0.212242  [89100/121447]\n",
      "loss: 0.112901  [92400/121447]\n",
      "loss: 0.185999  [95700/121447]\n",
      "loss: 0.221564  [99000/121447]\n",
      "loss: 0.184202  [102300/121447]\n",
      "loss: 0.460940  [105600/121447]\n",
      "loss: 0.323289  [108900/121447]\n",
      "loss: 0.274015  [112200/121447]\n",
      "loss: 0.265903  [115500/121447]\n",
      "loss: 0.162523  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.818820, Avg loss: 0.493303 \n",
      "\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0157e-04.\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.789524  [    0/121447]\n",
      "loss: 0.260390  [ 3300/121447]\n",
      "loss: 0.190227  [ 6600/121447]\n",
      "loss: 0.148601  [ 9900/121447]\n",
      "loss: 0.178393  [13200/121447]\n",
      "loss: 0.120374  [16500/121447]\n",
      "loss: 0.135732  [19800/121447]\n",
      "loss: 0.396983  [23100/121447]\n",
      "loss: 0.150378  [26400/121447]\n",
      "loss: 0.128864  [29700/121447]\n",
      "loss: 0.095044  [33000/121447]\n",
      "loss: 0.099688  [36300/121447]\n",
      "loss: 0.214326  [39600/121447]\n",
      "loss: 0.150404  [42900/121447]\n",
      "loss: 0.651736  [46200/121447]\n",
      "loss: 0.263754  [49500/121447]\n",
      "loss: 0.117471  [52800/121447]\n",
      "loss: 0.126073  [56100/121447]\n",
      "loss: 0.151905  [59400/121447]\n",
      "loss: 0.147704  [62700/121447]\n",
      "loss: 0.123366  [66000/121447]\n",
      "loss: 0.236340  [69300/121447]\n",
      "loss: 0.278330  [72600/121447]\n",
      "loss: 0.254838  [75900/121447]\n",
      "loss: 0.324102  [79200/121447]\n",
      "loss: 0.277212  [82500/121447]\n",
      "loss: 0.169098  [85800/121447]\n",
      "loss: 0.127975  [89100/121447]\n",
      "loss: 0.118499  [92400/121447]\n",
      "loss: 0.131122  [95700/121447]\n",
      "loss: 0.085777  [99000/121447]\n",
      "loss: 0.375407  [102300/121447]\n",
      "loss: 0.110840  [105600/121447]\n",
      "loss: 0.551185  [108900/121447]\n",
      "loss: 0.103792  [112200/121447]\n",
      "loss: 0.072097  [115500/121447]\n",
      "loss: 0.167376  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.823398, Avg loss: 0.505318 \n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.131654  [    0/121447]\n",
      "loss: 0.075470  [ 3300/121447]\n",
      "loss: 0.211648  [ 6600/121447]\n",
      "loss: 0.237302  [ 9900/121447]\n",
      "loss: 0.200132  [13200/121447]\n",
      "loss: 0.245825  [16500/121447]\n",
      "loss: 0.199633  [19800/121447]\n",
      "loss: 0.222397  [23100/121447]\n",
      "loss: 0.121555  [26400/121447]\n",
      "loss: 0.193189  [29700/121447]\n",
      "loss: 0.131936  [33000/121447]\n",
      "loss: 0.157716  [36300/121447]\n",
      "loss: 0.365269  [39600/121447]\n",
      "loss: 0.386804  [42900/121447]\n",
      "loss: 0.124177  [46200/121447]\n",
      "loss: 0.471875  [49500/121447]\n",
      "loss: 0.106320  [52800/121447]\n",
      "loss: 0.201984  [56100/121447]\n",
      "loss: 0.149017  [59400/121447]\n",
      "loss: 0.388821  [62700/121447]\n",
      "loss: 0.123413  [66000/121447]\n",
      "loss: 0.178537  [69300/121447]\n",
      "loss: 0.592054  [72600/121447]\n",
      "loss: 0.211346  [75900/121447]\n",
      "loss: 0.115916  [79200/121447]\n",
      "loss: 0.137273  [82500/121447]\n",
      "loss: 0.223647  [85800/121447]\n",
      "loss: 0.089739  [89100/121447]\n",
      "loss: 0.257017  [92400/121447]\n",
      "loss: 0.470488  [95700/121447]\n",
      "loss: 0.115039  [99000/121447]\n",
      "loss: 0.355031  [102300/121447]\n",
      "loss: 0.118794  [105600/121447]\n",
      "loss: 0.114279  [108900/121447]\n",
      "loss: 0.195033  [112200/121447]\n",
      "loss: 0.238351  [115500/121447]\n",
      "loss: 0.465493  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.825769, Avg loss: 0.528279 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.131069  [    0/121447]\n",
      "loss: 0.111964  [ 3300/121447]\n",
      "loss: 0.223327  [ 6600/121447]\n",
      "loss: 0.200242  [ 9900/121447]\n",
      "loss: 0.262191  [13200/121447]\n",
      "loss: 0.186486  [16500/121447]\n",
      "loss: 0.453037  [19800/121447]\n",
      "loss: 0.218353  [23100/121447]\n",
      "loss: 0.288171  [26400/121447]\n",
      "loss: 0.255821  [29700/121447]\n",
      "loss: 0.180744  [33000/121447]\n",
      "loss: 0.208707  [36300/121447]\n",
      "loss: 0.166226  [39600/121447]\n",
      "loss: 0.084878  [42900/121447]\n",
      "loss: 0.089970  [46200/121447]\n",
      "loss: 0.158384  [49500/121447]\n",
      "loss: 0.135928  [52800/121447]\n",
      "loss: 0.127288  [56100/121447]\n",
      "loss: 0.397931  [59400/121447]\n",
      "loss: 0.081537  [62700/121447]\n",
      "loss: 0.116701  [66000/121447]\n",
      "loss: 0.107222  [69300/121447]\n",
      "loss: 0.542303  [72600/121447]\n",
      "loss: 0.278864  [75900/121447]\n",
      "loss: 0.054332  [79200/121447]\n",
      "loss: 0.143594  [82500/121447]\n",
      "loss: 0.300854  [85800/121447]\n",
      "loss: 0.348353  [89100/121447]\n",
      "loss: 0.347279  [92400/121447]\n",
      "loss: 0.099769  [95700/121447]\n",
      "loss: 0.107237  [99000/121447]\n",
      "loss: 0.102187  [102300/121447]\n",
      "loss: 0.220388  [105600/121447]\n",
      "loss: 0.114288  [108900/121447]\n",
      "loss: 0.058008  [112200/121447]\n",
      "loss: 0.120069  [115500/121447]\n",
      "loss: 0.277537  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.822080, Avg loss: 0.516282 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.153186  [    0/121447]\n",
      "loss: 0.366186  [ 3300/121447]\n",
      "loss: 0.295248  [ 6600/121447]\n",
      "loss: 0.199546  [ 9900/121447]\n",
      "loss: 0.106718  [13200/121447]\n",
      "loss: 0.084389  [16500/121447]\n",
      "loss: 0.133155  [19800/121447]\n",
      "loss: 0.157717  [23100/121447]\n",
      "loss: 0.111672  [26400/121447]\n",
      "loss: 0.119721  [29700/121447]\n",
      "loss: 0.627858  [33000/121447]\n",
      "loss: 0.270696  [36300/121447]\n",
      "loss: 0.190068  [39600/121447]\n",
      "loss: 0.211590  [42900/121447]\n",
      "loss: 0.190164  [46200/121447]\n",
      "loss: 0.225969  [49500/121447]\n",
      "loss: 0.127026  [52800/121447]\n",
      "loss: 0.131194  [56100/121447]\n",
      "loss: 0.167527  [59400/121447]\n",
      "loss: 0.162892  [62700/121447]\n",
      "loss: 0.155396  [66000/121447]\n",
      "loss: 0.089291  [69300/121447]\n",
      "loss: 0.176986  [72600/121447]\n",
      "loss: 0.303115  [75900/121447]\n",
      "loss: 0.381621  [79200/121447]\n",
      "loss: 0.264963  [82500/121447]\n",
      "loss: 0.373096  [85800/121447]\n",
      "loss: 0.308741  [89100/121447]\n",
      "loss: 0.220100  [92400/121447]\n",
      "loss: 0.426724  [95700/121447]\n",
      "loss: 0.495900  [99000/121447]\n",
      "loss: 0.312729  [102300/121447]\n",
      "loss: 0.387962  [105600/121447]\n",
      "loss: 0.208439  [108900/121447]\n",
      "loss: 0.226764  [112200/121447]\n",
      "loss: 0.087037  [115500/121447]\n",
      "loss: 0.172872  [118800/121447]\n",
      "Test Error: \n",
      " Accuracy: 0.816448, Avg loss: 0.536387 \n",
      "\n",
      "CPU times: user 20min 16s, sys: 5min, total: 25min 16s\n",
      "Wall time: 25min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    verbose=150,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b87e43c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T09:19:30.437533Z",
     "iopub.status.busy": "2023-05-06T09:19:30.436944Z",
     "iopub.status.idle": "2023-05-06T09:19:30.443320Z",
     "shell.execute_reply": "2023-05-06T09:19:30.442509Z"
    },
    "papermill": {
     "duration": 0.062653,
     "end_time": "2023-05-06T09:19:30.445165",
     "exception": false,
     "start_time": "2023-05-06T09:19:30.382512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5c0778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T09:19:30.527276Z",
     "iopub.status.busy": "2023-05-06T09:19:30.526759Z",
     "iopub.status.idle": "2023-05-06T09:19:39.850307Z",
     "shell.execute_reply": "2023-05-06T09:19:39.848609Z"
    },
    "papermill": {
     "duration": 9.367239,
     "end_time": "2023-05-06T09:19:39.852667",
     "exception": false,
     "start_time": "2023-05-06T09:19:30.485428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.82      0.78     11815\n",
      "    positive       0.88      0.81      0.84     18547\n",
      "\n",
      "    accuracy                           0.82     30362\n",
      "   macro avg       0.81      0.82      0.81     30362\n",
      "weighted avg       0.82      0.82      0.82     30362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(net, test_dataloader, DEVICE)\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[\"negative\", \"positive\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec95de55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T09:19:39.942243Z",
     "iopub.status.busy": "2023-05-06T09:19:39.941959Z",
     "iopub.status.idle": "2023-05-06T09:19:39.947755Z",
     "shell.execute_reply": "2023-05-06T09:19:39.946862Z"
    },
    "papermill": {
     "duration": 0.051484,
     "end_time": "2023-05-06T09:19:39.949705",
     "exception": false,
     "start_time": "2023-05-06T09:19:39.898221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(\n",
    "        review: str,\n",
    "        target: str,\n",
    "        model: nn.Module,\n",
    "        vocab: ReviewsVocab,\n",
    "        target_names: list[str],\n",
    "        device: str = \"cpu\",\n",
    "):\n",
    "    x = vocab.encode(preprocess_review(review))\n",
    "    x = x.to(device)\n",
    "\n",
    "    pred = model(x.unsqueeze(0))\n",
    "    pred_proba, pred_label_idx = F.softmax(pred, 1).max(dim=1)\n",
    "    pred_label = target_names[pred_label_idx.cpu()]\n",
    "\n",
    "    print(f\"Review : {review}\")\n",
    "    print(f\"True   : {target}\")\n",
    "    print(f\"Predict: {pred_label} ({pred_proba.item():.2f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de6b3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T09:19:40.032311Z",
     "iopub.status.busy": "2023-05-06T09:19:40.031525Z",
     "iopub.status.idle": "2023-05-06T09:19:40.056113Z",
     "shell.execute_reply": "2023-05-06T09:19:40.054824Z"
    },
    "papermill": {
     "duration": 0.068107,
     "end_time": "2023-05-06T09:19:40.058160",
     "exception": false,
     "start_time": "2023-05-06T09:19:39.990053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review : Not good movie\n",
      "True   : negative\n",
      "Predict: positive (0.96)\n",
      "\n",
      "Review : boring\n",
      "True   : negative\n",
      "Predict: negative (0.96)\n",
      "\n",
      "Review : best movie ever\n",
      "True   : positive\n",
      "Predict: positive (0.94)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    (\"Not good movie\", \"negative\"),\n",
    "    (\"boring\", \"negative\"),\n",
    "    (\"best movie ever\", \"positive\"),\n",
    "]\n",
    "for review, target in reviews:\n",
    "    inference(\n",
    "        review=review,\n",
    "        target=target,\n",
    "        model=net,\n",
    "        vocab=vocab,\n",
    "        target_names=[\"negative\", \"positive\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "# не такой тупой оказывается, но это хуже чем log классификация не знаю почему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1fc24",
   "metadata": {
    "papermill": {
     "duration": 0.040817,
     "end_time": "2023-05-06T09:19:40.139765",
     "exception": false,
     "start_time": "2023-05-06T09:19:40.098948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2195.54969,
   "end_time": "2023-05-06T09:19:43.217002",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-06T08:43:07.667312",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
